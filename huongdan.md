# HÆ¯á»šNG DáºªN Äá»’ ÃN NHáº¬N Dáº NG NHáº C Cá»¤ - IRMAS DATASET

## ğŸ“‹ Tá»”NG QUAN Äá»’ ÃN

**TÃªn Ä‘á»“ Ã¡n:** Nháº­n dáº¡ng nháº¡c cá»¥ tá»« tÃ­n hiá»‡u Ã¢m thanh sá»­ dá»¥ng Deep Learning

**Má»¥c tiÃªu:** XÃ¢y dá»±ng mÃ´ hÃ¬nh CNN cÃ³ kháº£ nÄƒng nháº­n dáº¡ng 11 loáº¡i nháº¡c cá»¥ tá»« file audio

**PhÆ°Æ¡ng phÃ¡p:** Segment-based CNN vá»›i Mel-Spectrogram vÃ  Focal Loss

**Dataset:** IRMAS (Iowa Recorded Music Assessment) - 11 loáº¡i nháº¡c cá»¥, ~6,705 file training

---

## ğŸ› ï¸ CÃ”NG NGHá»† Sá»¬ Dá»¤NG

### ThÆ° viá»‡n chÃ­nh:

- **TensorFlow/Keras**: Deep Learning framework
- **Librosa**: Xá»­ lÃ½ audio vÃ  trÃ­ch xuáº¥t Mel-Spectrogram
- **NumPy, Pandas**: Xá»­ lÃ½ dá»¯ liá»‡u
- **Scikit-learn**: Chia dá»¯ liá»‡u, Ä‘Ã¡nh giÃ¡ model
- **Matplotlib, Seaborn**: Visualization

### MÃ´i trÆ°á»ng:

- **Google Colab** (khuyáº¿n nghá»‹): GPU runtime
- **Python 3.8+**

### CÃ i Ä‘áº·t dependencies:

**Cho Google Colab:**

```python
!pip install librosa>=0.9.0
!pip install tensorflow>=2.10.0
!pip install scikit-learn>=1.0.0
!pip install numpy>=1.21.0
!pip install pandas
!pip install matplotlib seaborn
!pip install joblib
```

**Cho mÃ´i trÆ°á»ng local:**

```bash
pip install -r requirements.txt
```

**File `requirements.txt` bao gá»“m:**

- numpy>=1.21.0
- soundfile>=0.10.0
- sounddevice>=0.4.0
- librosa>=0.9.0
- scikit-learn>=1.0.0
- tensorflow>=2.10.0
- scipy>=1.9.0
- joblib>=1.1.0
- Pillow>=9.0.0

---

## ğŸ“Š SÆ  LÆ¯á»¢C CÃC BÆ¯á»šC

Äá»“ Ã¡n gá»“m **6 STEP chÃ­nh**:

1. **STEP 1: Dá»® LIá»†U (IRMAS DATASET)** - Giá»›i thiá»‡u vÃ  cáº¥u trÃºc dataset
2. **STEP 2: Xá»¬ LÃ Dá»® LIá»†U** - Chia dá»¯ liá»‡u, trÃ­ch xuáº¥t segments, Mel-Spectrogram
3. **STEP 3: XÃ‚Y Dá»°NG MODEL** - Kiáº¿n trÃºc CNN vá»›i Focal Loss
4. **STEP 4: TRAINING MODEL** - Data Augmentation, Callbacks, Training process
5. **STEP 5: ÄÃNH GIÃ MODEL** - Metrics, Testing Data, Confusion Matrix
6. **STEP 6: á»¨NG Dá»¤NG THá»°C Táº¾** - Load model, Real-time Recognition, GUI Application

---

============

## ğŸ“ STEP 1: Dá»® LIá»†U (IRMAS DATASET)

### 1.1. Giá»›i thiá»‡u IRMAS Dataset

**IRMAS (Iowa Recorded Music Assessment)** lÃ  dataset chuáº©n cho bÃ i toÃ¡n nháº­n dáº¡ng nháº¡c cá»¥:

- **11 loáº¡i nháº¡c cá»¥**: Cello, Clarinet, Flute, Acoustic Guitar, Electric Guitar, Organ, Piano, Saxophone, Trumpet, Violin, Voice
- **Training Data**: ~6,705 file audio (má»—i file ~3 giÃ¢y)
- **Testing Data**: ~2,874 file audio (Ä‘á»™ dÃ i khÃ¡c nhau)

### 1.2. Cáº¥u trÃºc dá»¯ liá»‡u

#### Training Data:

```
IRMAS-TrainingData/
â”œâ”€â”€ cel/          (388 files)  - Cello
â”œâ”€â”€ cla/          (505 files)  - Clarinet
â”œâ”€â”€ flu/          (451 files)  - Flute
â”œâ”€â”€ gac/          (637 files)  - Acoustic Guitar
â”œâ”€â”€ gel/          (760 files)  - Electric Guitar
â”œâ”€â”€ org/          (682 files)  - Organ
â”œâ”€â”€ pia/          (721 files)  - Piano
â”œâ”€â”€ sax/          (626 files)  - Saxophone
â”œâ”€â”€ tru/          (577 files)  - Trumpet
â”œâ”€â”€ vio/          (580 files)  - Violin
â””â”€â”€ voi/          (778 files)  - Voice
```

**Äáº·c Ä‘iá»ƒm:**

- Má»—i file audio ~3 giÃ¢y
- ÄÃ£ Ä‘Æ°á»£c chia sáºµn theo tá»«ng thÆ° má»¥c nháº¡c cá»¥
- Format: `.wav`, sample rate: 22050 Hz
- **Single-label**: Má»—i file chá»‰ thuá»™c 1 nháº¡c cá»¥

#### Testing Data:

```
IRMAS-TestingData-Part1/
IRMAS-TestingData-Part2/
IRMAS-TestingData-Part3/
â”œâ”€â”€ [file].wav    - File audio
â””â”€â”€ [file].txt    - File chá»©a labels (cÃ³ thá»ƒ multi-label)
```

**Äáº·c Ä‘iá»ƒm:**

- Äá»™ dÃ i file khÃ¡c nhau (khÃ´ng cá»‘ Ä‘á»‹nh 3s)
- **Multi-label**: Má»™t file cÃ³ thá»ƒ cÃ³ nhiá»u nháº¡c cá»¥
- Labels Ä‘Æ°á»£c lÆ°u trong file `.txt` cÃ¹ng tÃªn

### 1.3. Mapping nháº¡c cá»¥

| Code | TÃªn Ä‘áº§y Ä‘á»§      | Sá»‘ lÆ°á»£ng (Training) |
| ---- | --------------- | ------------------- |
| cel  | Cello           | 388                 |
| cla  | Clarinet        | 505                 |
| flu  | Flute           | 451                 |
| gac  | Acoustic Guitar | 637                 |
| gel  | Electric Guitar | 760                 |
| org  | Organ           | 682                 |
| pia  | Piano           | 721                 |
| sax  | Saxophone       | 626                 |
| tru  | Trumpet         | 577                 |
| vio  | Violin          | 580                 |
| voi  | Voice           | 778                 |

============

## ğŸ”§ STEP 2: Xá»¬ LÃ Dá»® LIá»†U

### 2.1. Táº£i vÃ  giáº£i nÃ©n dataset

**Link táº£i IRMAS Dataset:**

- IRMAS Training Data: [Link táº£i TrainingData]
- IRMAS Testing Data: [Link táº£i TestingData-Part1, Part2, Part3]

**LÆ°u Ã½:** Dataset cÃ³ thá»ƒ táº£i tá»« trang chá»§ IRMAS hoáº·c cÃ¡c nguá»“n academic khÃ¡c.

**Cáº¥u trÃºc thÆ° má»¥c sau khi giáº£i nÃ©n:**

```
IRMAS/
â”œâ”€â”€ TrainingData/
â”‚   â”œâ”€â”€ [cel]/
â”‚   â”œâ”€â”€ [cla]/
â”‚   â”œâ”€â”€ [flu]/
â”‚   â”œâ”€â”€ [gac]/
â”‚   â”œâ”€â”€ [gel]/
â”‚   â”œâ”€â”€ [org]/
â”‚   â”œâ”€â”€ [pia]/
â”‚   â”œâ”€â”€ [sax]/
â”‚   â”œâ”€â”€ [tru]/
â”‚   â”œâ”€â”€ [vio]/
â”‚   â””â”€â”€ [voi]/
â””â”€â”€ TestingData/
    â”œâ”€â”€ Part1/
    â”œâ”€â”€ Part2/
    â””â”€â”€ Part3/
```

### 2.2. Giáº£i nÃ©n dataset

```python
# Giáº£i nÃ©n cÃ¡c file zip
extract_zip('IRMAS-TrainingData.zip', WORK_DIR)
extract_zip('IRMAS-TestingData-Part1.zip', WORK_DIR)
extract_zip('IRMAS-TestingData-Part2.zip', WORK_DIR)
extract_zip('IRMAS-TestingData-Part3.zip', WORK_DIR)
```

### 2.2. Load vÃ  chia dá»¯ liá»‡u

**Quan trá»ng:** Chia dá»¯ liá»‡u á»Ÿ cáº¥p Ä‘á»™ **FILE** trÆ°á»›c khi cáº¯t segments (trÃ¡nh data leakage)

**âš ï¸ LÆ¯U Ã QUAN TRá»ŒNG: CÃ³ 2 loáº¡i Test khÃ¡c nhau!**

1. **Test tá»« TrainingData** (chia tá»« 6,705 files):

   - DÃ¹ng Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ model trong quÃ¡ trÃ¬nh phÃ¡t triá»ƒn
   - Äáº£m báº£o model khÃ´ng overfit

2. **TestingData riÃªng** (2,874 files):
   - Dataset riÃªng biá»‡t, **KHÃ”NG náº±m trong TrainingData**
   - DÃ¹ng Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ cuá»‘i cÃ¹ng trÃªn data hoÃ n toÃ n má»›i
   - ÄÃ¢y lÃ  "real-world" test

**Quy trÃ¬nh chia TrainingData (6,705 files) thÃ nh 2 bÆ°á»›c:**

**BÆ°á»›c 1:** Chia Training Data thÃ nh Train+Val (80%) vÃ  Test (20%)

```python
# BÆ°á»›c 1: Chia 80% train+val vÃ  20% test
train_files, test_files, train_labels, test_labels = train_test_split(
    file_paths, labels,
    test_size=0.2,        # 20% cho test (tá»« TrainingData)
    random_state=42,
    stratify=labels      # Giá»¯ tá»· lá»‡ cÃ¡c class
)
```

**Káº¿t quáº£:**

- **Train + Val**: 80% = 5,364 files
- **Test (tá»« TrainingData)**: 20% = 1,341 files

**BÆ°á»›c 2:** Chia Train+Val thÃ nh Train (85%) vÃ  Val (15%)

```python
# BÆ°á»›c 2: Chia train+val thÃ nh train (85%) vÃ  val (15%)
train_files, val_files, train_labels, val_labels = train_test_split(
    train_files, train_labels,
    test_size=0.15,       # 15% cho val
    random_state=42,
    stratify=train_labels
)
```

**Káº¿t quáº£ cuá»‘i cÃ¹ng tá»« TrainingData:**

- **Train**: 85% Ã— 80% = **68%** (~4,559 files)
- **Val**: 15% Ã— 80% = **12%** (~805 files)
- **Test (tá»« TrainingData)**: **20%** (~1,341 files)

**Tá»•ng: 100% (6,705 files tá»« TrainingData)**

**TestingData riÃªng (2,874 files):**

- **KHÃ”NG** Ä‘Æ°á»£c dÃ¹ng trong quÃ¡ trÃ¬nh training
- Chá»‰ dÃ¹ng Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ cuá»‘i cÃ¹ng sau khi model Ä‘Ã£ train xong
- ÄÃ¢y lÃ  dataset "unseen" - model chÆ°a tá»«ng tháº¥y

**âš ï¸ QUAN TRá»ŒNG: Khi nÃ o dÃ¹ng tá»«ng set?**

**Trong quÃ¡ trÃ¬nh Training (má»—i epoch):**

```python
history = model.fit(
    train_gen_seg,                    # Train set â†’ DÃ¹ng Ä‘á»ƒ train
    validation_data=(X_val_seg, ...),  # Val set â†’ DÃ¹ng Má»–I EPOCH Ä‘á»ƒ monitor
    epochs=100,
    callbacks=[...]                    # EarlyStopping dá»±a trÃªn val_accuracy
)
```

- **Train set**: DÃ¹ng Ä‘á»ƒ train model (má»—i epoch)
- **Val set**: DÃ¹ng Má»–I EPOCH Ä‘á»ƒ:
  - Monitor `val_accuracy` vÃ  `val_loss`
  - Chá»n best model (ModelCheckpoint)
  - EarlyStopping náº¿u khÃ´ng cáº£i thiá»‡n
  - ReduceLROnPlateau náº¿u loss khÃ´ng giáº£m

**SAU KHI Training xong:**

- **Test set (tá»« TrainingData)**: DÃ¹ng Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ model sau khi train xong

  - KhÃ´ng dÃ¹ng trong quÃ¡ trÃ¬nh training
  - Chá»‰ dÃ¹ng 1 láº§n sau khi training hoÃ n táº¥t
  - Äá»ƒ kiá»ƒm tra model cÃ³ overfit khÃ´ng

- **TestingData riÃªng**: DÃ¹ng Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ cuá»‘i cÃ¹ng trÃªn data hoÃ n toÃ n má»›i

**TÃ³m táº¯t:**

| Set                        | Khi nÃ o dÃ¹ng               | Má»¥c Ä‘Ã­ch                                 |
| -------------------------- | -------------------------- | ---------------------------------------- |
| **Train**                  | Má»—i epoch                  | Train model                              |
| **Val**                    | Má»—i epoch                  | Monitor, chá»n best model, early stopping |
| **Test (tá»« TrainingData)** | Sau khi train xong (1 láº§n) | ÄÃ¡nh giÃ¡ model, kiá»ƒm tra overfit         |
| **TestingData riÃªng**      | Sau khi train xong (1 láº§n) | ÄÃ¡nh giÃ¡ cuá»‘i cÃ¹ng trÃªn data má»›i         |

**Táº¡i sao chia 2 bÆ°á»›c?**

- Äáº£m báº£o test set Ä‘á»™c láº­p hoÃ n toÃ n
- Val set dÃ¹ng Ä‘á»ƒ chá»n best model trong quÃ¡ trÃ¬nh training (má»—i epoch)
- Test set (tá»« TrainingData) dÃ¹ng Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ SAU KHI training xong (khÃ´ng dÃ¹ng má»—i epoch)
- TestingData riÃªng dÃ¹ng Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ cuá»‘i cÃ¹ng trÃªn data hoÃ n toÃ n má»›i

### 2.3. Segment-based Strategy

**Táº¡i sao dÃ¹ng Segment-based?**

- Dataset nhá» (~6,705 files) â†’ Cáº§n tÄƒng data
- Audio 3s â†’ Cáº¯t thÃ nh nhiá»u segments â†’ TÄƒng data Ã—6 láº§n
- Segment 2.0s â†’ Capture tá»‘t hÆ¡n nháº¡c cá»¥ sustain (organ, violin)
- Prediction robust hÆ¡n nhá» **weighted average aggregation** (segment cÃ³ confidence cao Ä‘Æ°á»£c Æ°u tiÃªn)

**Cáº¥u hÃ¬nh:**

```python
SEGMENT_DURATION = 2.0      # Äá»™ dÃ i má»—i segment (giÃ¢y)
NUM_SEGMENTS_TRAIN = 6      # Sá»‘ segments random cho má»—i file khi train
SEGMENT_OVERLAP = 0.5       # Overlap 50% cho sliding window khi test
```

**Aggregation Strategy:**

- **Training**: Random segments â†’ TÄƒng diversity, tÄƒng data
- **Validation/Testing**: Sliding window â†’ **Weighted Average** (segment cÃ³ confidence cao hÆ¡n Ä‘Æ°á»£c Æ°u tiÃªn)

**CÃ¡ch hoáº¡t Ä‘á»™ng:**

#### Training: Random Segments

**Tá»« file audio 3s â†’ Cáº¯t 6 segments ngáº«u nhiÃªn, má»—i segment 2.0s**

```
File audio: [==========] (3 giÃ¢y)
            |--2s--|  |--2s--|  |--2s--|  ... (6 segments random)
```

**VÃ­ dá»¥:**

- File audio 3s (66,150 samples vá»›i sr=22050)
- Cáº¯t 6 segments, má»—i segment 2.0s (44,100 samples)
- CÃ¡c segments cÃ³ thá»ƒ **overlap** (chá»“ng lÃªn nhau) hoáº·c **khÃ´ng overlap**
- Vá»‹ trÃ­ báº¯t Ä‘áº§u cá»§a má»—i segment lÃ  **ngáº«u nhiÃªn**

```python
def extract_random_segments(y, num_segments=6):
    """
    Cáº¯t 6 segments ngáº«u nhiÃªn tá»« audio 3s
    Má»—i segment dÃ i 2.0s
    """
    segments = []
    for _ in range(6):
        start = np.random.randint(0, max_start)  # Vá»‹ trÃ­ ngáº«u nhiÃªn
        segment = y[start:start + 44100]  # 2.0s = 44100 samples
        segments.append(segment)
    return segments
```

**Káº¿t quáº£:** 1 file â†’ 6 segments â†’ Data tÄƒng Ã—6

#### Validation/Testing: Sliding Window

**Tá»« file audio â†’ Cáº¯t segments vá»›i sliding window (overlap 50%)**

```
File audio: [==========] (3 giÃ¢y)
            |--2s--|
                 |--2s--|  (overlap 50%)
                      |--2s--|
```

**VÃ­ dá»¥ vá»›i file 3s:**

- Segment 1: 0.0s â†’ 2.0s
- Segment 2: 1.0s â†’ 3.0s (overlap 50% vá»›i segment 1)
- **Káº¿t quáº£:** 2 segments tá»« 1 file

**VÃ­ dá»¥ vá»›i file 5s:**

- Segment 1: 0.0s â†’ 2.0s
- Segment 2: 1.0s â†’ 3.0s
- Segment 3: 2.0s â†’ 4.0s
- Segment 4: 3.0s â†’ 5.0s
- **Káº¿t quáº£:** 4 segments tá»« 1 file

**CÃ´ng thá»©c tÃ­nh sá»‘ segments vá»›i sliding window:**

```
hop = segment_samples Ã— (1 - overlap)
sá»‘_segments = âŒŠ(audio_length - segment_samples) / hopâŒ‹ + 1
```

Trong Ä‘Ã³:

- `hop`: BÆ°á»›c nháº£y giá»¯a cÃ¡c segments
- `overlap`: Tá»· lá»‡ overlap (0.5 = 50%)
- `segment_samples`: Sá»‘ samples trong 1 segment
- `audio_length`: Äá»™ dÃ i audio (samples)

**VÃ­ dá»¥ vá»›i audio 5s (110,250 samples) vÃ  segment 2s (44,100 samples), overlap 50%:**

- `hop = 44,100 Ã— (1 - 0.5) = 22,050 samples`
- `sá»‘_segments = âŒŠ(110,250 - 44,100) / 22,050âŒ‹ + 1 = âŒŠ3âŒ‹ + 1 = 4 segments`

```python
def extract_sliding_segments(y, overlap=0.5):
    """
    Cáº¯t segments vá»›i sliding window
    Overlap 50% â†’ Má»—i segment má»›i báº¯t Ä‘áº§u tá»« giá»¯a segment trÆ°á»›c
    """
    segments = []
    step = int(segment_samples * (1 - overlap))  # BÆ°á»›c nháº£y = 50% segment
    for start in range(0, len(y) - segment_samples, step):
        segment = y[start:start + segment_samples]
        segments.append(segment)
    return segments
```

**Káº¿t quáº£:**

- **Train**: 4,559 files â†’ **27,354 segments** (Ã—6)
- **Val**: 805 files â†’ **1,610 segments** (Ã—2 trung bÃ¬nh)
- **Test**: 1,341 files â†’ **2,682 segments** (Ã—2 trung bÃ¬nh)

**Táº¡i sao khÃ¡c nhau?**

- **Training**: Random 6 segments â†’ TÄƒng diversity, tÄƒng data
- **Val/Test**: Sliding window â†’ Cover toÃ n bá»™ audio, Ä‘áº£m báº£o consistency
- **Aggregation**: DÃ¹ng **Weighted Average** (khÃ´ng pháº£i simple average) â†’ Segment cÃ³ confidence cao Ä‘Æ°á»£c Æ°u tiÃªn

### 2.4. TrÃ­ch xuáº¥t Mel-Spectrogram

**Mel-Spectrogram** lÃ  biá»ƒu diá»…n táº§n sá»‘ cá»§a Ã¢m thanh theo thá»i gian:

- Chuyá»ƒn Ä‘á»•i audio signal â†’ 2D image (frequency Ã— time)
- Model CNN xá»­ lÃ½ nhÆ° áº£nh grayscale

#### 2.4.1. CÃ¡c khÃ¡i niá»‡m cÆ¡ báº£n

**Audio Signal (1D):**

```
Audio: [sample1, sample2, sample3, ..., sampleN]
       â†‘
       TÃ­n hiá»‡u sá»‘ hÃ³a theo thá»i gian
```

**Spectrogram (2D):**

```
Frequency (Hz)
    â†‘
    |  [intensity]
    |  [intensity]
    |  [intensity]
    |  ...
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Time
```

#### 2.4.2. CÃ¡c tham sá»‘ quan trá»ng

**Tham sá»‘:**

```python
SAMPLE_RATE = 22050        # Hz - Táº§n sá»‘ láº¥y máº«u
N_MELS = 128               # Sá»‘ mel filter banks (mel bins)
N_FFT = 2048               # FFT window size
HOP_LENGTH = 512           # Hop length giá»¯a cÃ¡c frames
```

**Giáº£i thÃ­ch tá»«ng tham sá»‘:**

##### 1. **SAMPLE_RATE (22050 Hz)**

- Sá»‘ samples láº¥y trong 1 giÃ¢y
- 22050 Hz = 22,050 samples/giÃ¢y
- Segment 2.0s = 44,100 samples

##### 2. **N_FFT (2048) - FFT Window Size**

- KÃ­ch thÆ°á»›c cá»­a sá»• FFT (Fast Fourier Transform)
- DÃ¹ng Ä‘á»ƒ chuyá»ƒn Ä‘á»•i tá»« time domain â†’ frequency domain
- **CÃ ng lá»›n â†’ Ä‘á»™ phÃ¢n giáº£i táº§n sá»‘ cÃ ng cao, nhÆ°ng Ä‘á»™ phÃ¢n giáº£i thá»i gian tháº¥p hÆ¡n**

**VÃ­ dá»¥:**

```
N_FFT = 2048 â†’ PhÃ¢n tÃ­ch 2048 samples má»—i láº§n
â†’ Táº§n sá»‘ tá»‘i Ä‘a cÃ³ thá»ƒ phÃ¢n tÃ­ch = SAMPLE_RATE / 2 = 11025 Hz
```

##### 3. **HOP_LENGTH (512) - BÆ°á»›c nháº£y giá»¯a cÃ¡c frames**

- **Quan trá»ng nháº¥t Ä‘á»ƒ hiá»ƒu táº¡i sao cÃ³ 87 time frames!**

**Hop Length lÃ  gÃ¬?**

- Khoáº£ng cÃ¡ch (sá»‘ samples) giá»¯a 2 cá»­a sá»• FFT liÃªn tiáº¿p
- Hop length = 512 â†’ Má»—i frame cÃ¡ch nhau 512 samples

**VÃ­ dá»¥:**

```
Frame 1: samples [0:2048]      (N_FFT = 2048)
Frame 2: samples [512:2560]    (nháº£y 512 samples = HOP_LENGTH)
Frame 3: samples [1024:3072]   (nháº£y thÃªm 512 samples)
Frame 4: samples [1536:3584]   (nháº£y thÃªm 512 samples)
...
```

**Táº¡i sao dÃ¹ng Hop Length?**

- Náº¿u khÃ´ng cÃ³ hop (hop = N_FFT): Máº¥t thÃ´ng tin, khÃ´ng overlap
- CÃ³ hop < N_FFT: CÃ³ overlap â†’ Capture tá»‘t hÆ¡n cÃ¡c thay Ä‘á»•i theo thá»i gian

##### 4. **N_MELS (128) - Sá»‘ Mel Bins**

**Mel Bins lÃ  gÃ¬?**

- **Mel scale**: Thang Ä‘o táº§n sá»‘ phÃ¹ há»£p vá»›i cÃ¡ch tai ngÆ°á»i cáº£m nháº­n
- Tai ngÆ°á»i nháº¡y cáº£m hÆ¡n vá»›i táº§n sá»‘ tháº¥p (200-2000 Hz) so vá»›i táº§n sá»‘ cao
- Mel scale: Chia táº§n sá»‘ thÃ nh cÃ¡c "bins" (thÃ¹ng) theo thang Mel

**CÃ´ng thá»©c chuyá»ƒn Ä‘á»•i Hz â†’ Mel:**

```
mel(f) = 2595 Â· logâ‚â‚€(1 + f/700)
```

Trong Ä‘Ã³:

- `f`: Táº§n sá»‘ (Hz)
- `mel(f)`: Táº§n sá»‘ theo thang Mel

**VÃ­ dá»¥:**

```
Táº§n sá»‘ tuyáº¿n tÃ­nh: 0Hz, 100Hz, 200Hz, 300Hz, ... (cÃ¡ch Ä‘á»u)
Mel scale:        0,   100,   200,   300,   ... (khÃ´ng cÃ¡ch Ä‘á»u, táº­p trung á»Ÿ táº§n sá»‘ tháº¥p)
```

**N_MELS = 128:**

- Chia táº§n sá»‘ thÃ nh 128 bins theo thang Mel
- Má»—i bin Ä‘áº¡i diá»‡n cho má»™t dáº£i táº§n sá»‘
- **128 mel bins = 128 hÃ ng trong mel-spectrogram**

#### 2.4.3. TÃ­nh toÃ¡n sá»‘ Time Frames (87)

**CÃ´ng thá»©c tÃ­nh sá»‘ time frames:**

```
Sá»‘ time frames = (Sá»‘ samples - N_FFT) / HOP_LENGTH + 1
```

**Vá»›i segment 2.0s:**

```python
SEGMENT_DURATION = 2.0      # giÃ¢y
SAMPLE_RATE = 22050         # Hz
SEGMENT_SAMPLES = 2.0 Ã— 22050 = 44,100 samples

N_FFT = 2048
HOP_LENGTH = 512

Sá»‘ time frames = (44,100 - 2048) / 512 + 1
                = 42,052 / 512 + 1
                = 82.13 + 1
                = 83.13
                â‰ˆ 87 frames (do lÃ m trÃ²n vÃ  padding)
```

**Giáº£i thÃ­ch:**

- Segment cÃ³ 44,100 samples
- Frame Ä‘áº§u tiÃªn: samples [0:2048]
- Frame cuá»‘i cÃ¹ng: samples [42,052:44,100]
- Sá»‘ frames = (44,100 - 2048) / 512 + 1 â‰ˆ 87

**Táº¡i sao +1?**

- Frame Ä‘áº§u tiÃªn báº¯t Ä‘áº§u á»Ÿ sample 0
- Má»—i frame tiáº¿p theo nháº£y 512 samples
- Cáº§n +1 Ä‘á»ƒ tÃ­nh frame Ä‘áº§u tiÃªn

#### 2.4.4. Input Shape: (128, 87, 1)

**Input shape:** `(128, 87, 1)`

- **128**: Sá»‘ mel bins (frequency axis - chiá»u cao)
- **87**: Sá»‘ time frames (time axis - chiá»u rá»™ng)
- **1**: Channel (grayscale - giá»‘ng áº£nh Ä‘en tráº¯ng)

**HÃ¬nh dung:**

```
Mel-Spectrogram (128 Ã— 87):

Frequency (128 bins)
    â†‘
128 | [intensity] [intensity] ... [intensity]  â† Frame 1
127 | [intensity] [intensity] ... [intensity]  â† Frame 2
... |    ...         ...      ...    ...
  1 | [intensity] [intensity] ... [intensity]
  0 | [intensity] [intensity] ... [intensity]  â† Frame 87
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Time
       0s    0.023s  0.046s  ...   2.0s
       (má»—i frame cÃ¡ch nhau 512/22050 â‰ˆ 0.023s)
```

#### 2.4.5. CÃ´ng thá»©c trÃ­ch xuáº¥t

```python
def segment_to_mel(segment):
    """
    Chuyá»ƒn Ä‘á»•i audio segment â†’ Mel-Spectrogram

    Input: segment (44,100 samples = 2.0s)
    Output: mel_spec_db (128 Ã— 87)
    """
    # 1. TÃ­nh Mel-Spectrogram
    mel_spec = librosa.feature.melspectrogram(
        y=segment,          # Audio signal (44,100 samples)
        sr=22050,           # Sample rate
        n_mels=128,         # Sá»‘ mel bins
        n_fft=2048,         # FFT window size
        hop_length=512      # Hop length giá»¯a cÃ¡c frames
    )
    # Output: mel_spec shape = (128, 87)

    # 2. Chuyá»ƒn sang decibel (dB)
    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)
    # GiÃ¡ trá»‹ tá»« -âˆ Ä‘áº¿n 0 dB (normalize vá» [0, 1] sau)

    return mel_spec_db
```

#### 2.4.6. TÃ³m táº¯t

| Tham sá»‘              | GiÃ¡ trá»‹  | Ã nghÄ©a                   |
| -------------------- | -------- | ------------------------- |
| **SAMPLE_RATE**      | 22050 Hz | Sá»‘ samples/giÃ¢y           |
| **N_FFT**            | 2048     | KÃ­ch thÆ°á»›c cá»­a sá»• FFT     |
| **HOP_LENGTH**       | 512      | BÆ°á»›c nháº£y giá»¯a cÃ¡c frames |
| **N_MELS**           | 128      | Sá»‘ mel bins (frequency)   |
| **SEGMENT_DURATION** | 2.0s     | Äá»™ dÃ i segment            |
| **SEGMENT_SAMPLES**  | 44,100   | Sá»‘ samples trong segment  |
| **Time Frames**      | 87       | Sá»‘ frames theo thá»i gian  |

**CÃ´ng thá»©c quan trá»ng:**

```
Time Frames = (SEGMENT_SAMPLES - N_FFT) / HOP_LENGTH + 1
            = (44,100 - 2048) / 512 + 1
            â‰ˆ 87
```

============

## ğŸ—ï¸ STEP 3: XÃ‚Y Dá»°NG MODEL

### 3.1. Kiáº¿n trÃºc Model

**CNN vá»›i cÃ¡c Conv Blocks:**

```
Input (128, 87, 1)
    â†“
Block 1: Conv2D(32) â†’ BatchNorm â†’ Conv2D(32) â†’ BatchNorm
    â†“ MaxPooling2D + Dropout(0.25)
Block 2: Conv2D(64) â†’ BatchNorm â†’ Conv2D(64) â†’ BatchNorm
    â†“ MaxPooling2D + Dropout(0.25)
Block 3: Conv2D(128) â†’ BatchNorm â†’ Conv2D(128) â†’ BatchNorm
    â†“ MaxPooling2D + Dropout(0.3)
Block 4: Conv2D(256) â†’ BatchNorm â†’ Conv2D(256) â†’ BatchNorm
    â†“ MaxPooling2D + Dropout(0.3)
Global Average Pooling
    â†“
Dense (512) â†’ BatchNorm â†’ Dropout(0.5)
    â†“
Dense (256) â†’ BatchNorm â†’ Dropout(0.5)
    â†“
Output (11 classes) - Softmax
```

**CÃ´ng thá»©c Softmax:**

```
p_i = exp(z_i) / Î£â±¼ exp(z_j)
```

Trong Ä‘Ã³:

- `z_i`: Logit (raw output) cá»§a class i
- `p_i`: Probability cá»§a class i sau softmax
- `Î£â±¼ p_j = 1` (tá»•ng probabilities = 1)

### 3.2. Loss Function: Focal Loss

**CÃ´ng thá»©c gá»‘c:**

```
FL(p_t) = -Î±_t Â· (1 - p_t)^Î³ Â· log(p_t)
```

Trong Ä‘Ã³:

- `p_t`: Probability cá»§a true class
  - `p_t = p` náº¿u y = 1
  - `p_t = 1 - p` náº¿u y = 0
- `Î±_t`: Class weight (trong code: khÃ´ng dÃ¹ng, Î± = 1)
- `Î³`: Focusing parameter (gamma = 2.0)

**CÃ´ng thá»©c Cross Entropy (CE) - cÆ¡ sá»Ÿ cá»§a Focal Loss:**

```
CE = -Î£ y_i Â· log(p_i)
```

Trong Ä‘Ã³:

- `y_i`: True label (one-hot encoding)
- `p_i`: Predicted probability cho class i
- Vá»›i multi-class: `CE = -log(p_t)` vá»›i `p_t` lÃ  probability cá»§a true class

**Focal Loss = Focal Weight Ã— Cross Entropy:**

```
FL = (1 - p_t)^Î³ Â· CE
   = (1 - p_t)^Î³ Â· (-log(p_t))
```

**Giáº£i thÃ­ch:**

- `(1 - p_t)^Î³`: Focal weight
  - `p_t` cao (dá»… predict) â†’ `(1 - p_t)^Î³` nhá» â†’ Weight tháº¥p
  - `p_t` tháº¥p (khÃ³ predict) â†’ `(1 - p_t)^Î³` lá»›n â†’ Weight cao
- Táº­p trung vÃ o cÃ¡c sample khÃ³ phÃ¢n biá»‡t

**Focal Loss** táº­p trung vÃ o cÃ¡c sample khÃ³ phÃ¢n biá»‡t:

- `gamma=2.0`: Táº­p trung vá»«a pháº£i vÃ o hard examples
- GiÃºp cáº£i thiá»‡n precision cho cÃ¡c class yáº¿u (Saxophone, Trumpet)

**Trong code:**

```python
def focal_loss(gamma=2.0):
    def focal_loss_fixed(y_true, y_pred):
        epsilon = keras.backend.epsilon()
        y_pred = keras.backend.clip(y_pred, epsilon, 1.0 - epsilon)

        # Cross entropy
        ce = -y_true * keras.backend.log(y_pred)

        # p_t: probability cá»§a true class
        p_t = keras.backend.sum(y_true * y_pred, axis=-1, keepdims=True)

        # Focal weight: (1 - p_t)^Î³
        focal_weight = keras.backend.pow((1 - p_t), gamma)

        # Focal Loss
        loss = focal_weight * ce
        return keras.backend.mean(loss)

    return focal_loss_fixed
```

**VÃ­ dá»¥:**

```
Sample dá»… (p_t = 0.9):
  FL = (1 - 0.9)Â² Ã— CE = 0.01 Ã— CE â†’ Weight ráº¥t tháº¥p

Sample khÃ³ (p_t = 0.3):
  FL = (1 - 0.3)Â² Ã— CE = 0.49 Ã— CE â†’ Weight cao hÆ¡n 49 láº§n
```

### 3.5. Regularization

**L2 Regularization (Weight Decay):**

**CÃ´ng thá»©c:**

```
L_total = L_loss + Î» Â· Î£ Î¸Â²
```

Trong Ä‘Ã³:

- `L_loss`: Loss function (Focal Loss)
- `Î»`: Regularization coefficient (0.001)
- `Î£ Î¸Â²`: Sum of squared weights

**Gradient vá»›i L2:**

```
âˆ‚L/âˆ‚Î¸ = âˆ‚L_loss/âˆ‚Î¸ + 2Î» Â· Î¸
```

**Batch Normalization:**

**CÃ´ng thá»©c:**

```
Î¼_B = (1/m) Â· Î£áµ¢â‚Œâ‚áµ x_i                    # Mean cá»§a batch
ÏƒÂ²_B = (1/m) Â· Î£áµ¢â‚Œâ‚áµ (x_i - Î¼_B)Â²         # Variance cá»§a batch
xÌ‚_i = (x_i - Î¼_B) / âˆš(ÏƒÂ²_B + Îµ)            # Normalize
y_i = Î³ Â· xÌ‚_i + Î²                           # Scale and shift
```

**CÃ´ng thá»©c Ä‘áº§y Ä‘á»§:**

1. **Mean:** `Î¼_B = (1/m) Â· Î£áµ¢â‚Œâ‚áµ x_i`
2. **Variance:** `ÏƒÂ²_B = (1/m) Â· Î£áµ¢â‚Œâ‚áµ (x_i - Î¼_B)Â²`
3. **Normalize:** `xÌ‚_i = (x_i - Î¼_B) / âˆš(ÏƒÂ²_B + Îµ)`
4. **Scale and Shift:** `y_i = Î³ Â· xÌ‚_i + Î²`

Trong Ä‘Ã³:

- `m`: Batch size
- `Îµ = 1e-5`: Epsilon (trÃ¡nh chia 0)
- `Î³`: Scale parameter (learnable)
- `Î²`: Shift parameter (learnable)

**Dropout:**

**CÃ´ng thá»©c:**

```
y_i = {
    x_i / (1 - p)  vá»›i xÃ¡c suáº¥t (1 - p)  # Giá»¯ láº¡i
    0              vá»›i xÃ¡c suáº¥t p         # Drop
}
```

Trong Ä‘Ã³:

- `p`: Dropout rate (0.25, 0.3, 0.5)
- Training: Ãp dá»¥ng dropout
- Inference: KhÃ´ng dropout (hoáº·c scale bá»Ÿi 1-p)

**TÃ³m táº¯t:**

- **L2 Regularization** (weight_decay=0.01): Giáº£m overfitting
- **BatchNormalization**: á»”n Ä‘á»‹nh training
- # **Dropout** (0.25-0.5): Giáº£m overfitting

## ğŸ¯ STEP 4: TRAINING MODEL

### 4.1. Data Augmentation

**âš ï¸ PHÃ‚N BIá»†T: Segment-based vs Data Augmentation**

**Segment-based (Ä‘Ã£ giáº£i thÃ­ch á»Ÿ Step 2.3):**

- **Cáº¯t audio signal** â†’ Táº¡o nhiá»u segments tá»« 1 file audio
- **Input**: Audio signal (1D array)
- **Output**: Nhiá»u segments (1 file â†’ 6 segments)
- **Má»¥c Ä‘Ã­ch**: TÄƒng sá»‘ lÆ°á»£ng data (1 file â†’ 8 samples)

**Data Augmentation (pháº§n nÃ y):**

- **Biáº¿n Ä‘á»•i mel-spectrogram** â†’ Táº¡o biáº¿n thá»ƒ tá»« 1 segment Ä‘Ã£ cÃ³
- **Input**: Mel-spectrogram (128 Ã— 87) Ä‘Ã£ cÃ³
- **Output**: Mel-spectrogram Ä‘Ã£ Ä‘Æ°á»£c biáº¿n Ä‘á»•i
- **Má»¥c Ä‘Ã­ch**: TÄƒng diversity, giáº£m overfitting

**Quy trÃ¬nh:**

```
1. Audio file (3s)
   â†“
2. Segment-based: Cáº¯t thÃ nh 6 segments (má»—i segment 2.0s)
   â†“
3. Má»—i segment â†’ Mel-spectrogram (128 Ã— 87)
   â†“
4. Data Augmentation: Biáº¿n Ä‘á»•i mel-spectrogram (SpecAugment, Mixup)
   â†“
5. Model training
```

**Táº¡i sao cáº§n Data Augmentation?**

- Dataset nhá» (~6,705 files) â†’ Cáº§n tÄƒng diversity
- Giáº£m overfitting â†’ Model generalize tá»‘t hÆ¡n
- TÄƒng robustness â†’ Model chá»‹u Ä‘Æ°á»£c noise, biáº¿n Ä‘á»•i

**2 ká»¹ thuáº­t Ä‘Æ°á»£c sá»­ dá»¥ng:**

1. **SpecAugment**: Che má»™t pháº§n mel-spectrogram
2. **Mixup**: Trá»™n 2 samples vá»›i nhau

#### 4.1.1. SpecAugment (Spectral Augmentation)

**SpecAugment** lÃ  ká»¹ thuáº­t augmentation cho spectrogram, tÆ°Æ¡ng tá»± nhÆ° Cutout cho áº£nh.

**CÃ¡ch hoáº¡t Ä‘á»™ng:**

**1. Time Masking** - Che theo trá»¥c thá»i gian:

```
Mel-Spectrogram gá»‘c (128 Ã— 87):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â”‚ â† Frequency bin 127
â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â”‚
â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â”‚
â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â”‚
â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â”‚
â”‚ ...                             â”‚
â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â”‚ â† Frequency bin 0
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    0s    0.5s   1.0s   1.5s   2.0s
         â†‘ Time axis (87 frames)

Sau Time Masking (che 10 frames tá»« frame 20):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â”‚ â† Che theo thá»i gian
â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â”‚
â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â”‚
â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â”‚
â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â”‚
â”‚ ...                             â”‚
â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    0s    0.5s   1.0s   1.5s   2.0s
         â†‘ Che tá»« frame 20-30
```

**2. Frequency Masking** - Che theo trá»¥c táº§n sá»‘:

```
Mel-Spectrogram gá»‘c (128 Ã— 87):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â”‚ â† Frequency bin 127
â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â”‚
â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â”‚
â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â”‚ â† Che 15 bins tá»« bin 50
â”‚ â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â”‚ â† Frequency bin 50-65
â”‚ â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â”‚
â”‚ â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â”‚
â”‚ ...                             â”‚
â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â”‚ â† Frequency bin 0
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    0s    0.5s   1.0s   1.5s   2.0s
```

**CÃ´ng thá»©c:**

**Time Masking:**

```
x[tâ‚€ : tâ‚€ + t, :] = 0
```

- `t`: Random tá»« [0, time_mask_param] (25 frames)
- `tâ‚€`: Random start position

**Frequency Masking:**

```
x[:, fâ‚€ : fâ‚€ + f] = 0
```

- `f`: Random tá»« [0, freq_mask_param] (20 bins)
- `fâ‚€`: Random start position

**Tham sá»‘ trong code:**

```python
def _spec_augment(self, batch,
                  time_mask_param=25,    # Che tá»‘i Ä‘a 25 frames
                  freq_mask_param=20,    # Che tá»‘i Ä‘a 20 frequency bins
                  num_masks=2):          # Ãp dá»¥ng 2 láº§n masking
    """
    SpecAugment: Time vÃ  Frequency Masking
    CÃ´ng thá»©c: x[tâ‚€:tâ‚€+t, :] = 0 (time), x[:, fâ‚€:fâ‚€+f] = 0 (frequency)
    """
    for i in range(len(batch)):
        for _ in range(num_masks):  # Ãp dá»¥ng 2 láº§n
            # Time masking: Che ngáº«u nhiÃªn 0-25 frames
            t = np.random.randint(0, time_mask_param)
            t0 = np.random.randint(0, max(1, time_steps - t))
            batch[i, t0:t0+t, :, :] = 0  # Set = 0 (che)

            # Frequency masking: Che ngáº«u nhiÃªn 0-20 bins
            f = np.random.randint(0, freq_mask_param)
            f0 = np.random.randint(0, max(1, freq_bins - f))
            batch[i, :, f0:f0+f, :] = 0  # Set = 0 (che)

    return batch
```

**Lá»£i Ã­ch:**

- Model há»c Ä‘Æ°á»£c cÃ¡c Ä‘áº·c trÆ°ng quan trá»ng, khÃ´ng phá»¥ thuá»™c vÃ o má»™t pháº§n cá»¥ thá»ƒ
- Robust vá»›i noise, máº¥t tÃ­n hiá»‡u táº¡m thá»i
- Giáº£m overfitting

**VÃ­ dá»¥ thá»±c táº¿:**

- Time masking: MÃ´ phá»ng tÃ­n hiá»‡u bá»‹ máº¥t trong thá»i gian ngáº¯n
- Frequency masking: MÃ´ phá»ng má»™t sá»‘ táº§n sá»‘ bá»‹ nhiá»…u

#### 4.1.2. Mixup Augmentation

**Mixup** lÃ  ká»¹ thuáº­t trá»™n 2 samples vá»›i nhau Ä‘á»ƒ táº¡o sample má»›i.

**CÃ¡ch hoáº¡t Ä‘á»™ng:**

**CÃ´ng thá»©c:**

```
x_mixed = Î» Ã— x1 + (1 - Î») Ã— x2
y_mixed = Î» Ã— y1 + (1 - Î») Ã— y2
```

Trong Ä‘Ã³:

- `Î»` (lambda) Ä‘Æ°á»£c láº¥y tá»« phÃ¢n phá»‘i Beta(Î±, Î±)
- `Î± = 0.2` (tham sá»‘ trong code)
- `x1, x2`: 2 mel-spectrograms
- `y1, y2`: 2 labels (one-hot encoding)

**CÃ´ng thá»©c gá»‘c:**

```
x_mixed = Î» Â· xâ‚ + (1 - Î») Â· xâ‚‚
y_mixed = Î» Â· yâ‚ + (1 - Î») Â· yâ‚‚
```

Trong Ä‘Ã³:

- `Î» ~ Beta(Î±, Î±)`: Mixing coefficient
- `Î± = 0.2`: Tham sá»‘ Beta distribution
- `xâ‚, xâ‚‚`: 2 samples ngáº«u nhiÃªn
- `yâ‚, yâ‚‚`: 2 labels tÆ°Æ¡ng á»©ng

**Beta Distribution:**

```
Beta(Î±, Î±) vá»›i Î± = 0.2
```

- `Î±` nhá» â†’ `Î»` thÆ°á»ng gáº§n 0 hoáº·c 1 (Ã­t khi á»Ÿ giá»¯a)
- `Î±` lá»›n â†’ `Î»` thÆ°á»ng á»Ÿ giá»¯a (0.5)

**VÃ­ dá»¥:**

```
Sample 1: Piano (label = [0,0,0,0,0,0,1,0,0,0,0])
Sample 2: Guitar (label = [0,0,0,1,0,0,0,0,0,0,0])

Î» = 0.3 (ngáº«u nhiÃªn tá»« Beta(0.2, 0.2))

x_mixed = 0.3 Ã— Piano_mel + 0.7 Ã— Guitar_mel
y_mixed = 0.3 Ã— [0,0,0,0,0,0,1,0,0,0,0] + 0.7 Ã— [0,0,0,1,0,0,0,0,0,0,0]
        = [0, 0, 0, 0.7, 0, 0, 0.3, 0, 0, 0, 0]
        = 70% Guitar + 30% Piano
```

**HÃ¬nh dung:**

```
Mel-Spectrogram 1 (Piano):        Mel-Spectrogram 2 (Guitar):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â”‚              â”‚ â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ â”‚
â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â”‚              â”‚ â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ â”‚
â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â”‚              â”‚ â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ â”‚
â”‚ ...             â”‚              â”‚ ...             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Mixup (Î» = 0.3):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“ â”‚ â† 30% Piano + 70% Guitar
â”‚ â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“ â”‚
â”‚ â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“ â”‚
â”‚ ...             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Code:**

```python
def _mixup(self, x, y, alpha=0.2):
    """
    Mixup augmentation
    alpha: Tham sá»‘ Beta distribution (alpha cÃ ng nhá» â†’ Î» gáº§n 0 hoáº·c 1)
    """
    batch_size = len(x)
    indices = np.random.permutation(batch_size)  # XÃ¡o trá»™n indices

    # Láº¥y Î» tá»« Beta distribution
    lam = np.random.beta(alpha, alpha)
    # alpha=0.2 â†’ Î» thÆ°á»ng gáº§n 0 hoáº·c 1 (Ã­t khi á»Ÿ giá»¯a)

    # Trá»™n 2 samples
    x_mixed = lam * x + (1 - lam) * x[indices]
    y_mixed = lam * y + (1 - lam) * y[indices]

    return x_mixed, y_mixed
```

**Lá»£i Ã­ch:**

- TÄƒng diversity: Táº¡o ra cÃ¡c sample má»›i khÃ´ng cÃ³ trong dataset
- Model há»c Ä‘Æ°á»£c ranh giá»›i giá»¯a cÃ¡c class tá»‘t hÆ¡n
- Giáº£m overfitting

**LÆ°u Ã½:**

- Chá»‰ Ã¡p dá»¥ng cho **training** (khÃ´ng Ã¡p dá»¥ng cho validation/test)
- Ãp dá»¥ng vá»›i xÃ¡c suáº¥t 50% (trong code: `if np.random.random() > 0.5`)

#### 4.1.3. Data Generator

**SegmentDataGenerator** káº¿t há»£p cáº£ 2 ká»¹ thuáº­t:

```python
class SegmentDataGenerator(keras.utils.Sequence):
    def __init__(self, x, y, batch_size=64,
                 augment=True,    # Báº­t SpecAugment
                 mixup=True):      # Báº­t Mixup
        self.x = x
        self.y = y
        self.augment = augment
        self.mixup = mixup

    def __getitem__(self, idx):
        # Láº¥y batch
        x_batch = self.x[batch_indices].copy()
        y_batch = self.y[batch_indices].copy()

        # 1. Ãp dá»¥ng SpecAugment (náº¿u báº­t)
        if self.augment:
            x_batch = self._spec_augment(x_batch)

        # 2. Ãp dá»¥ng Mixup (náº¿u báº­t, vá»›i xÃ¡c suáº¥t 50%)
        if self.mixup and np.random.random() > 0.5:
            x_batch, y_batch = self._mixup(x_batch, y_batch)

        return x_batch, y_batch
```

**Sá»­ dá»¥ng:**

```python
# Training: CÃ³ augmentation
train_gen = SegmentDataGenerator(
    X_train_seg, y_train_cat_seg,
    batch_size=64,
    augment=True,      # SpecAugment
    mixup=True         # Mixup
)

# Validation: KHÃ”NG cÃ³ augmentation
val_gen = SegmentDataGenerator(
    X_val_seg, y_val_cat_seg,
    batch_size=64,
    augment=False,     # KhÃ´ng augmentation
    mixup=False,       # KhÃ´ng mixup
    shuffle=False      # KhÃ´ng shuffle
)
```

**Táº¡i sao validation khÃ´ng cÃ³ augmentation?**

- Validation dÃ¹ng Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ model trÃªn data "tháº­t"
- Augmentation chá»‰ dÃ¹ng Ä‘á»ƒ train model tá»‘t hÆ¡n
- Äáº£m báº£o Ä‘Ã¡nh giÃ¡ chÃ­nh xÃ¡c

#### 4.1.4. TÃ³m táº¯t

| Ká»¹ thuáº­t        | CÃ¡ch hoáº¡t Ä‘á»™ng                                | Lá»£i Ã­ch                               | Ãp dá»¥ng cho             |
| --------------- | --------------------------------------------- | ------------------------------------- | ----------------------- |
| **SpecAugment** | Che má»™t pháº§n mel-spectrogram (time/frequency) | Robust vá»›i noise, giáº£m overfitting    | Training                |
| **Mixup**       | Trá»™n 2 samples vá»›i nhau                       | TÄƒng diversity, há»c ranh giá»›i tá»‘t hÆ¡n | Training (50% xÃ¡c suáº¥t) |

**Káº¿t quáº£:**

- TÄƒng diversity cá»§a training data
- Model generalize tá»‘t hÆ¡n
- Giáº£m overfitting
- TÄƒng accuracy trÃªn test set

### 4.2. Callbacks

**EarlyStopping:**

- Monitor: `val_accuracy`
- Patience: 15 epochs
- Restore best weights

**ReduceLROnPlateau:**

- Monitor: `val_loss`
- Factor: 0.5 (giáº£m 50%)
- Patience: 7 epochs
- Min LR: 1e-7

**ModelCheckpoint:**

- Save best model dá»±a trÃªn `val_accuracy`
- Path: `/content/best_segment_cnn.keras`

### 4.3. Training Configuration

```python
EPOCHS_SEG = 100
BATCH_SIZE = 64
LEARNING_RATE = 0.001
OPTIMIZER = AdamW(learning_rate=0.001, weight_decay=0.01)
LOSS = Focal Loss (gamma=2.0)
CLASS_WEIGHT = 'balanced'  # TÃ­nh class weights Ä‘á»ƒ xá»­ lÃ½ class imbalance
```

**Class Weights:**

- TÃ­nh toÃ¡n class weights Ä‘á»ƒ xá»­ lÃ½ class imbalance trong dataset
- DÃ¹ng `compute_class_weight('balanced', ...)` tá»« scikit-learn
- Class cÃ³ Ã­t samples sáº½ cÃ³ weight cao hÆ¡n

```python
from sklearn.utils.class_weight import compute_class_weight
class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)
class_weight_dict = {i: w for i, w in enumerate(class_weights)}
```

**Training process:**

1. Load data generators vá»›i augmentation
2. Compile model vá»›i Focal Loss
3. Fit vá»›i callbacks vÃ  class_weight
4. Model tá»± Ä‘á»™ng save best weights

**Thá»i gian training:** ~2-3 giá» trÃªn GPU Colab (100 epochs)

============

## ğŸ“ˆ STEP 5: ÄÃNH GIÃ MODEL

### 5.1. Metrics

**Accuracy:** Tá»· lá»‡ dá»± Ä‘oÃ¡n Ä‘Ãºng
**Precision:** Tá»· lá»‡ dá»± Ä‘oÃ¡n Ä‘Ãºng trong sá»‘ cÃ¡c dá»± Ä‘oÃ¡n
**Recall:** Tá»· lá»‡ tÃ¬m Ä‘Æ°á»£c trong sá»‘ cÃ¡c máº«u thá»±c táº¿
**F1-Score:** Trung bÃ¬nh Ä‘iá»u hÃ²a cá»§a Precision vÃ  Recall

### 5.2. ÄÃ¡nh giÃ¡ trÃªn Testing Data

**Testing Data cÃ³ Ä‘áº·c Ä‘iá»ƒm:**

- Multi-label (má»™t file cÃ³ thá»ƒ cÃ³ nhiá»u nháº¡c cá»¥)
- Äá»™ dÃ i file khÃ¡c nhau
- KhÃ´ng cÃ³ trong training set

**Quy trÃ¬nh:**

1. Load audio file
2. Extract segments (sliding window)
3. Predict tá»«ng segment
4. **Weighted Average**: Segment cÃ³ confidence cao hÆ¡n Ä‘Æ°á»£c Æ°u tiÃªn
5. Final prediction

```python
# Weighted average aggregation
segment_weights = np.max(segment_probs, axis=1)  # Confidence
segment_weights = segment_weights / segment_weights.sum()
avg_probs = np.average(segment_probs, axis=0, weights=segment_weights)
```

### 5.3. Káº¿t quáº£ mong Ä‘á»£i

**Accuracy:** ~79-81% trÃªn Testing Data

**CÃ¡c class tá»‘t:**

- Flute, Acoustic Guitar, Electric Guitar, Piano: Precision > 70%

**CÃ¡c class khÃ³:**

- Saxophone, Trumpet: Precision tháº¥p (dá»… nháº§m vá»›i nhau)
- Organ, Clarinet: Precision tháº¥p (Ã­t data)

### 5.4. Confusion Matrix

PhÃ¢n tÃ­ch confusion matrix Ä‘á»ƒ xem:

- Class nÃ o dá»… nháº§m vá»›i nhau
- Class nÃ o cáº§n cáº£i thiá»‡n

**VÃ­ dá»¥ nháº§m láº«n phá»• biáº¿n:**

- Saxophone â†’ Trumpet (vÃ  ngÆ°á»£c láº¡i)
- Violin â†’ Voice
- Organ â†’ Piano

============

## ğŸ’» STEP 6: á»¨NG Dá»¤NG THá»°C Táº¾

### 6.1. Load Model

```python
import keras
import joblib

# Load model vá»›i custom_objects cho Focal Loss
model = keras.models.load_model(
    'IRMAS_Models/best_segment_cnn.keras',
    custom_objects={'focal_loss_fixed': focal_loss(gamma=2.0)}
)

# Load label encoder
label_encoder = joblib.load('IRMAS_Models/label_encoder_seg.joblib')
```

### 6.2. Quy trÃ¬nh Real-time Recognition

**Khi cháº¡y á»©ng dá»¥ng thá»±c táº¿, model sáº½ lÃ m gÃ¬ Ä‘á»ƒ nháº­n biáº¿t nháº¡c cá»¥?**

#### 6.2.1. Quy trÃ¬nh Ä‘áº§y Ä‘á»§

```
1. Input Audio (tá»« file hoáº·c microphone)
   â†“
2. Preprocessing (Resample, Normalize)
   â†“
3. Extract Segments (Sliding Window)
   â†“
4. Convert to Mel-Spectrogram
   â†“
5. Predict tá»«ng Segment
   â†“
6. Weighted Average Aggregation
   â†“
7. Final Prediction + Confidence
```

#### 6.2.2. Chi tiáº¿t tá»«ng bÆ°á»›c

**BÆ°á»›c 1: Load Audio**

```python
# Tá»« file hoáº·c record tá»« microphone
audio, sr = librosa.load('audio.wav', sr=22050)
# Hoáº·c
audio = record_from_microphone()  # Record 3-5 giÃ¢y
```

**BÆ°á»›c 2: Preprocessing**

```python
# Resample vá» 22050 Hz (náº¿u cáº§n)
if sample_rate != 22050:
    audio = librosa.resample(audio, orig_sr=sample_rate, target_sr=22050)

# Normalize vá» [-1, 1]
if audio.max() > 1.0 or audio.min() < -1.0:
    audio = audio / np.max(np.abs(audio))
```

**âš ï¸ Táº¡i sao cáº§n normalize audio?**

**Váº¥n Ä‘á»:**

- Audio tá»« cÃ¡c nguá»“n khÃ¡c nhau cÃ³ amplitude khÃ¡c nhau
- File A: amplitude [-0.5, 0.5]
- File B: amplitude [-2.0, 2.0]
- File C: amplitude [-0.1, 0.1]

**Náº¿u khÃ´ng normalize:**

- Model sáº½ bá»‹ áº£nh hÆ°á»Ÿng bá»Ÿi volume (amplitude)
- File cÃ³ volume cao â†’ Mel-spectrogram cÃ³ giÃ¡ trá»‹ lá»›n â†’ Model nghÄ© lÃ  "quan trá»ng hÆ¡n"
- File cÃ³ volume tháº¥p â†’ Mel-spectrogram cÃ³ giÃ¡ trá»‹ nhá» â†’ Model nghÄ© lÃ  "Ã­t quan trá»ng"

**Sau khi normalize:**

- Táº¥t cáº£ audio Ä‘á»u cÃ³ amplitude [-1, 1]
- Model chá»‰ táº­p trung vÃ o **pattern** (hÃ¬nh dáº¡ng sÃ³ng), khÃ´ng phá»¥ thuá»™c vÃ o **volume**
- Robust vá»›i cÃ¡c má»©c volume khÃ¡c nhau

**VÃ­ dá»¥:**

```
TrÆ°á»›c normalize:
File A: [0.1, 0.2, 0.3, ...] â†’ Mel-spec: giÃ¡ trá»‹ nhá»
File B: [0.5, 1.0, 1.5, ...] â†’ Mel-spec: giÃ¡ trá»‹ lá»›n
â†’ Model nghÄ© File B "quan trá»ng hÆ¡n" (SAI!)

Sau normalize:
File A: [0.2, 0.4, 0.6, ...] â†’ Mel-spec: giÃ¡ trá»‹ tÆ°Æ¡ng Ä‘Æ°Æ¡ng
File B: [0.2, 0.4, 0.6, ...] â†’ Mel-spec: giÃ¡ trá»‹ tÆ°Æ¡ng Ä‘Æ°Æ¡ng
â†’ Model chá»‰ nhÃ¬n vÃ o pattern (ÄÃšNG!)
```

**BÆ°á»›c 3: Extract Segments (Sliding Window)**

```python
def extract_sliding_segments(audio, segment_duration=2.0, overlap=0.5):
    """
    Cáº¯t audio thÃ nh segments vá»›i sliding window
    Overlap 50% â†’ Cover toÃ n bá»™ audio
    """
    segments = []
    segment_samples = int(22050 * segment_duration)  # 44,100 samples
    hop = int(segment_samples * (1 - overlap))  # 22,050 samples (50% overlap)

    start = 0
    while start + segment_samples <= len(audio):
        segment = audio[start:start + segment_samples]
        segments.append(segment)
        start += hop  # Nháº£y 50% segment

    return segments
```

**VÃ­ dá»¥ vá»›i audio 5s:**

```
Audio: [==========] (5 giÃ¢y)
       |--2s--|      â† Segment 1 (0.0s â†’ 2.0s)
            |--2s--| â† Segment 2 (1.0s â†’ 3.0s, overlap 50%)
                 |--2s--| â† Segment 3 (2.0s â†’ 4.0s)
                      |--2s--| â† Segment 4 (3.0s â†’ 5.0s)

Káº¿t quáº£: 4 segments
```

**BÆ°á»›c 4: Convert to Mel-Spectrogram**

```python
def segment_to_mel(segment):
    """
    Chuyá»ƒn má»—i segment â†’ Mel-Spectrogram (128 Ã— 87)
    """
    # 1. TÃ­nh Mel-Spectrogram (power spectrum)
    mel_spec = librosa.feature.melspectrogram(
        y=segment,
        sr=22050,
        n_mels=128,
        n_fft=2048,
        hop_length=512
    )
    # Output: GiÃ¡ trá»‹ power (cÃ³ thá»ƒ ráº¥t lá»›n, vÃ­ dá»¥: 0 â†’ 1000000)

    # 2. Chuyá»ƒn sang decibel (dB) vÃ  normalize
    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)
    # Output: GiÃ¡ trá»‹ dB (thÆ°á»ng tá»« -80 dB Ä‘áº¿n 0 dB)
    # ref=np.max â†’ Normalize vá» [min, 0], vá»›i max = 0 dB

    return mel_spec_db

# Convert táº¥t cáº£ segments
mel_segments = [segment_to_mel(seg) for seg in segments]
mel_segments = np.array(mel_segments)[..., np.newaxis]  # Shape: (N, 128, 87, 1)
```

**âš ï¸ Táº¡i sao cáº§n `power_to_db` vá»›i `ref=np.max`?**

**Váº¥n Ä‘á» vá»›i Mel-Spectrogram gá»‘c:**

- Mel-spectrogram cÃ³ giÃ¡ trá»‹ **power** (cÃ³ thá»ƒ ráº¥t lá»›n: 0 â†’ 1,000,000)
- GiÃ¡ trá»‹ phÃ¢n bá»‘ khÃ´ng Ä‘á»u â†’ KhÃ³ cho model há»c
- Model dá»… bá»‹ áº£nh hÆ°á»Ÿng bá»Ÿi giÃ¡ trá»‹ tuyá»‡t Ä‘á»‘i

**Giáº£i phÃ¡p: `power_to_db` vá»›i `ref=np.max`:**

```python
mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)
```

**CÃ¡ch hoáº¡t Ä‘á»™ng:**

1. Chuyá»ƒn power â†’ decibel (dB): `dB = 10 * log10(power)`
2. `ref=np.max`: Láº¥y max cá»§a mel-spectrogram lÃ m reference
3. Normalize: `dB_normalized = dB - max_dB`
4. Káº¿t quáº£: GiÃ¡ trá»‹ tá»« `-âˆ` Ä‘áº¿n `0` dB (max = 0 dB)

**VÃ­ dá»¥:**

```
Mel-Spectrogram gá»‘c (power):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1000  500  200 â”‚ â† GiÃ¡ trá»‹ lá»›n, khÃ´ng Ä‘á»u
â”‚  500  300  100 â”‚
â”‚  200  100   50 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Sau power_to_db (ref=np.max):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   0   -3   -7  â”‚ â† GiÃ¡ trá»‹ dB, normalize vá» [min, 0]
â”‚  -3   -5   -10 â”‚
â”‚  -7  -10  -13  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Lá»£i Ã­ch:**

- GiÃ¡ trá»‹ trong khoáº£ng há»£p lÃ½ (thÆ°á»ng -80 dB Ä‘áº¿n 0 dB)
- Normalize vá» cÃ¹ng scale â†’ Model há»c tá»‘t hÆ¡n
- Táº­p trung vÃ o **tÆ°Æ¡ng Ä‘á»‘i** (relative), khÃ´ng phá»¥ thuá»™c vÃ o **tuyá»‡t Ä‘á»‘i** (absolute)
- Giá»‘ng vá»›i cÃ¡ch tai ngÆ°á»i cáº£m nháº­n (logarithmic scale)

**BÆ°á»›c 5: Predict tá»«ng Segment**

```python
# Predict táº¥t cáº£ segments cÃ¹ng lÃºc (batch processing)
segment_probs = model.predict(mel_segments, batch_size=32, verbose=0)
# Output: (N, 11) - N segments, 11 classes
# Má»—i hÃ ng lÃ  probability distribution cho 1 segment
```

**VÃ­ dá»¥ vá»›i 4 segments:**

```
Segment 1: [0.01, 0.02, 0.05, 0.80, 0.03, ...] â†’ Guitar (80%)
Segment 2: [0.01, 0.03, 0.04, 0.75, 0.05, ...] â†’ Guitar (75%)
Segment 3: [0.02, 0.01, 0.06, 0.82, 0.02, ...] â†’ Guitar (82%)
Segment 4: [0.01, 0.02, 0.05, 0.78, 0.04, ...] â†’ Guitar (78%)
```

**BÆ°á»›c 6: Weighted Average Aggregation**

**Táº¡i sao cáº§n Weighted Average?**

- KhÃ´ng pháº£i táº¥t cáº£ segments Ä‘á»u cÃ³ cháº¥t lÆ°á»£ng nhÆ° nhau
- Segment cÃ³ confidence cao â†’ ÄÃ¡ng tin cáº­y hÆ¡n
- Segment cÃ³ confidence tháº¥p â†’ CÃ³ thá»ƒ bá»‹ nhiá»…u

**CÃ´ng thá»©c gá»‘c (Weighted Average):**

```
P_final = Î£áµ¢ (w_i Â· P_i) / Î£áµ¢ w_i
```

VÃ¬ weights Ä‘Ã£ Ä‘Æ°á»£c normalize (Î£áµ¢ w_i = 1), cÃ´ng thá»©c Ä‘Æ¡n giáº£n hÃ³a thÃ nh:

```
P_final = Î£áµ¢ (w_i Â· P_i)
```

Trong Ä‘Ã³:

- `P_i`: Probability vector cá»§a segment i (shape: 11 classes)
- `w_i`: Weight cá»§a segment i (Ä‘Ã£ normalize, Î£áµ¢ w_i = 1)
- `P_final`: Final probability vector

**Weight calculation:**

```
w_i = max(P_i) / Î£â±¼ max(P_j)
```

- `max(P_i)`: Confidence cá»§a segment i (probability cao nháº¥t)
- Normalize Ä‘á»ƒ tá»•ng weights = 1.0: `w_i = w_i / Î£â±¼ w_j`

**CÃ¡ch tÃ­nh:**

```python
# 1. TÃ­nh confidence cá»§a má»—i segment (max probability)
segment_weights = np.max(segment_probs, axis=1)
# VÃ­ dá»¥: [0.80, 0.75, 0.82, 0.78] â†’ Confidence cá»§a má»—i segment

# 2. Normalize weights (tá»•ng = 1)
segment_weights = segment_weights / (segment_weights.sum() + 1e-10)
# VÃ­ dá»¥: [0.25, 0.24, 0.26, 0.25] â†’ Trá»ng sá»‘ cá»§a má»—i segment

# 3. Weighted average: P_final = Î£(w_i Â· P_i)
avg_probs = np.average(segment_probs, axis=0, weights=segment_weights)
# Káº¿t quáº£: [0.01, 0.02, 0.05, 0.79, 0.03, ...] â†’ Final probabilities
```

**âš ï¸ Táº¡i sao cáº§n normalize weights?**

**Váº¥n Ä‘á»:**

- Confidence cá»§a cÃ¡c segments: [0.80, 0.75, 0.82, 0.78]
- Tá»•ng = 3.15 (khÃ´ng pháº£i 1.0)
- Náº¿u dÃ¹ng trá»±c tiáº¿p â†’ Weighted average sáº½ bá»‹ scale lÃªn 3.15 láº§n (SAI!)

**Sau khi normalize:**

- [0.80, 0.75, 0.82, 0.78] â†’ [0.25, 0.24, 0.26, 0.25]
- Tá»•ng = 1.0 â†’ ÄÃºng vá»›i Ä‘á»‹nh nghÄ©a "trá»ng sá»‘" (weights)

**VÃ­ dá»¥:**

```
TrÆ°á»›c normalize:
Weights: [0.80, 0.75, 0.82, 0.78] (tá»•ng = 3.15)
â†’ Weighted average sáº½ bá»‹ scale lÃªn 3.15 láº§n (SAI!)

Sau normalize:
Weights: [0.25, 0.24, 0.26, 0.25] (tá»•ng = 1.0)
â†’ Weighted average Ä‘Ãºng (ÄÃšNG!)
```

**Lá»£i Ã­ch:**

- Äáº£m báº£o tá»•ng weights = 1.0 â†’ Káº¿t quáº£ Ä‘Ãºng
- Segment cÃ³ confidence cao â†’ Weight cao hÆ¡n
- Segment cÃ³ confidence tháº¥p â†’ Weight tháº¥p hÆ¡n

**VÃ­ dá»¥ cá»¥ thá»ƒ:**

```
Segment 1: Guitar 80% confidence â†’ Weight 0.25
Segment 2: Guitar 75% confidence â†’ Weight 0.24
Segment 3: Guitar 82% confidence â†’ Weight 0.26 (cao nháº¥t)
Segment 4: Guitar 78% confidence â†’ Weight 0.25

Weighted Average:
Guitar = 0.25Ã—0.80 + 0.24Ã—0.75 + 0.26Ã—0.82 + 0.25Ã—0.78 = 0.79 (79%)
```

**BÆ°á»›c 7: Final Prediction**

```python
# Láº¥y class cÃ³ probability cao nháº¥t
predicted_idx = np.argmax(avg_probs)
predicted_label = label_encoder.inverse_transform([predicted_idx])[0]
confidence = avg_probs[predicted_idx] * 100

# Top-3 predictions
top3_indices = np.argsort(avg_probs)[::-1][:3]
```

**VÃ­ dá»¥ káº¿t quáº£:**

```
Predicted: Acoustic Guitar
Confidence: 79.23%

Top 3:
1. Acoustic Guitar: 79.23%
2. Electric Guitar: 12.45%
3. Piano: 5.32%
```

#### 6.2.3. Code Ä‘áº§y Ä‘á»§

```python
def predict_audio_file(audio_path, model, label_encoder):
    """
    Predict nháº¡c cá»¥ tá»« audio file
    """
    # 1. Load audio
    audio, sr = librosa.load(audio_path, sr=22050)

    # 2. Preprocessing
    if sr != 22050:
        audio = librosa.resample(audio, orig_sr=sr, target_sr=22050)
    if audio.max() > 1.0:
        audio = audio / np.max(np.abs(audio))

    # 3. Extract segments (sliding window, overlap 50%)
    segments = extract_sliding_segments(audio, segment_duration=2.0, overlap=0.5)

    # 4. Convert to mel-spectrogram
    mel_segments = [segment_to_mel(seg) for seg in segments]
    mel_segments = np.array(mel_segments)[..., np.newaxis]

    # 5. Predict
    segment_probs = model.predict(mel_segments, batch_size=32, verbose=0)

    # 6. Weighted average
    segment_weights = np.max(segment_probs, axis=1)  # Confidence
    segment_weights = segment_weights / (segment_weights.sum() + 1e-10)
    avg_probs = np.average(segment_probs, axis=0, weights=segment_weights)

    # 7. Final prediction
    predicted_idx = np.argmax(avg_probs)
    predicted_label = label_encoder.inverse_transform([predicted_idx])[0]
    confidence = avg_probs[predicted_idx] * 100

    return predicted_label, confidence, avg_probs
```

#### 6.2.4. Táº¡i sao dÃ¹ng Sliding Window + Weighted Average?

**Sliding Window:**

- Cover toÃ n bá»™ audio (khÃ´ng bá» sÃ³t pháº§n nÃ o)
- Overlap 50% â†’ Äáº£m báº£o khÃ´ng máº¥t thÃ´ng tin á»Ÿ ranh giá»›i

**Weighted Average:**

- Segment cÃ³ confidence cao â†’ ÄÃ¡ng tin cáº­y hÆ¡n
- Segment cÃ³ confidence tháº¥p â†’ CÃ³ thá»ƒ bá»‹ nhiá»…u â†’ Ãt áº£nh hÆ°á»Ÿng
- Káº¿t quáº£ robust hÆ¡n so vá»›i simple average

**VÃ­ dá»¥ so sÃ¡nh:**

```
Simple Average:
Segment 1: Guitar 80% + Segment 2: Guitar 75% â†’ Guitar 77.5%

Weighted Average:
Segment 1: Guitar 80% (weight 0.6) + Segment 2: Guitar 75% (weight 0.4)
â†’ Guitar 78% (Segment 1 cÃ³ confidence cao hÆ¡n â†’ weight cao hÆ¡n)
```

### 6.3. Test Model Script

File `test_model.py` cung cáº¥p script test nhanh Ä‘á»ƒ kiá»ƒm tra:

- Model cÃ³ load Ä‘Æ°á»£c khÃ´ng (vá»›i `custom_objects` cho Focal Loss)
- Label encoder cÃ³ load Ä‘Æ°á»£c khÃ´ng
- Segment config cÃ³ load Ä‘Æ°á»£c khÃ´ng
- Prediction vá»›i dummy data cÃ³ hoáº¡t Ä‘á»™ng khÃ´ng

**Cháº¡y test:**

```bash
python test_model.py
```

**LÆ°u Ã½:** Script nÃ y cáº§n Ä‘á»‹nh nghÄ©a hÃ m `focal_loss` Ä‘á»ƒ load model thÃ nh cÃ´ng.

### 6.4. GUI Application

File `instrument_recognition_program.py` cung cáº¥p:

- Giao diá»‡n Ä‘á»“ há»a (Tkinter)
- Record audio tá»« microphone
- Load file audio
- Predict vÃ  hiá»ƒn thá»‹ káº¿t quáº£
- Top-3 predictions vá»›i confidence

**Cháº¡y á»©ng dá»¥ng:**

```bash
python instrument_recognition_program.py
```

============

## ğŸ“ TÃ“M Táº®T CÃC BÆ¯á»šC

1. **Chuáº©n bá»‹ dá»¯ liá»‡u**

   - Táº£i IRMAS dataset
   - Giáº£i nÃ©n vÃ o Google Drive
   - Mount Drive trong Colab

2. **Xá»­ lÃ½ dá»¯ liá»‡u**

   - Load files tá»« cÃ¡c thÆ° má»¥c
   - Chia train/val/test á»Ÿ cáº¥p Ä‘á»™ file
   - TrÃ­ch xuáº¥t segments (random cho train, sliding cho val/test)
   - Convert sang mel-spectrogram

3. **XÃ¢y dá»±ng model**

   - CNN vá»›i 4 Conv Blocks (32 â†’ 64 â†’ 128 â†’ 256 filters)
   - Batch Normalization vÃ  Dropout Ä‘á»ƒ trÃ¡nh overfitting
   - Focal Loss (gamma=2.0)

4. **Training**

   - Data augmentation (SpecAugment + Mixup)
   - Class weights Ä‘á»ƒ xá»­ lÃ½ class imbalance
   - Callbacks (EarlyStopping, ReduceLROnPlateau, ModelCheckpoint)
   - Train 100 epochs

5. **ÄÃ¡nh giÃ¡**

   - Test trÃªn TestingData
   - PhÃ¢n tÃ­ch confusion matrix
   - Classification report

6. **á»¨ng dá»¥ng**
   - Load model vÃ  predict
   - Sá»­ dá»¥ng trong GUI application

---

## âš ï¸ LÆ¯U Ã QUAN TRá»ŒNG

1. **Data Leakage**: LuÃ´n chia train/test á»Ÿ cáº¥p Ä‘á»™ FILE trÆ°á»›c khi cáº¯t segments
2. **GPU Runtime**: Báº¯t buá»™c dÃ¹ng GPU trong Colab Ä‘á»ƒ training nhanh
3. **Focal Loss**: Pháº£i cung cáº¥p `custom_objects` khi load model
4. **Segment Aggregation**: DÃ¹ng weighted average (khÃ´ng pháº£i simple mean)
5. **Model Saving**: LÆ°u model vÃ o Google Drive Ä‘á»ƒ khÃ´ng máº¥t khi runtime disconnect

---

## ğŸ“ Cáº¤U TRÃšC THÆ¯ Má»¤C PROJECT

```
Musical_Instrument_Detection/
â”œâ”€â”€ IRMAS_Training_CNN.ipynb       # Notebook training chÃ­nh
â”œâ”€â”€ instrument_recognition_program.py  # á»¨ng dá»¥ng GUI
â”œâ”€â”€ test_model.py                  # Script test model
â”œâ”€â”€ requirements.txt               # Dependencies
â”œâ”€â”€ huongdan.md                   # File hÆ°á»›ng dáº«n nÃ y
â”œâ”€â”€ README_DEMO.md                # HÆ°á»›ng dáº«n demo
â”œâ”€â”€ QUICK_START.md                # Quick start guide
â”œâ”€â”€ IRMAS_Models/                 # ThÆ° má»¥c chá»©a models
â”‚   â”œâ”€â”€ best_segment_cnn.keras
â”‚   â”œâ”€â”€ label_encoder_seg.joblib
â”‚   â””â”€â”€ segment_config.joblib
â””â”€â”€ IRMAS/                        # Dataset (sau khi táº£i vÃ  giáº£i nÃ©n)
    â”œâ”€â”€ TrainingData/
    â””â”€â”€ TestingData/
```

---

## ğŸ”— TÃ€I LIá»†U THAM KHáº¢O

- **IRMAS Dataset**: [Link dataset - cáº§n cáº­p nháº­t link cá»¥ thá»ƒ]
- **Focal Loss Paper**: "Focal Loss for Dense Object Detection" (Lin et al., 2017)
- **Librosa Documentation**: https://librosa.org/
- **TensorFlow/Keras Documentation**: https://www.tensorflow.org/api_docs

---

**ChÃºc báº¡n thÃ nh cÃ´ng vá»›i Ä‘á»“ Ã¡n! ğŸµğŸ¸ğŸ¹**
